seed: 0

# Wandb
project: # todo
entity: # todo

# Training
lr: 0.001
weight_decay: 0.1

t_lr: 0.0      # time embedding lr
z_lr: 0.0      # positional embedding lr
deltas_lr: 0.0 # modulation lr
implicit_filter_lr:  0.001
implicit_filter_weight_decay: 0.0

lr_schedule: "cosine"
total_steps: 50000
warmup_steps: 5000
save_interval: 100000
log_interval: 150

# Data
dataset: "icl_synthetics"
num_examples: 4000
num_test_examples: 500
vocab_size: 40
copy_method: "assoc_recall"
input_seq_len: 131072
l_max: 131074 

data_kwargs:
    seed: 0
    batch_size: 16
    data_dir: # todo

rng_keys: ["dropout"]

# Model
model: 'hyena_simplelm'
d_model: 64
n_layer: 2
d_inner: 256

layer: "S5_operator"
layer_kwargs:
    ssm_size: 32
    ssm_blocks: 16
    order: 2
    num_heads: 1
    inner_factor: 1
    num_blocks: 1
    outer_mixing: False
    drop_rate: 0.0
    filter_dropout: 0.0
    filter_cls: 'hyena_S5'
    post_order_ffn: False
    short_filter_order: 3
    activation_type: "id"
    return_state: False
    filter_args:
        C_init: "complex_normal"
        dt_min: 0.0001
        dt_max: 0.1
        conj_sym: True
        clip_eigs: True
        activation: "half_glu2"

seq_model_kwargs:
    max_position_embeddings: 0
    resid_dropout: 0.0
    embed_dropout: 0.0
    layer_norm_epsilon: 0.00001
    pad_vocab_size_multiple: 1
